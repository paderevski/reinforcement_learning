{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ff733e-1854-432c-823b-d96415334147",
   "metadata": {},
   "source": [
    "## Gridworld A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac44865d-8551-436f-8da2-fd5878d9e352",
   "metadata": {},
   "source": [
    "We experiment with policy and value incrementing algorithms on the simple gridworld problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5fb81-e74b-418c-9d02-1404b8e3fd94",
   "metadata": {},
   "source": [
    "Gridworld is a closed 4x4 grid in which an agent may move any cardinal direction. The NW and SE corners are terminal states from which no more transitions are allowed. The goal of the agent is to learn a policy which maximized the expected discounted reward at each step. The reward function returns -1 for all non-terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cdf6711-4d6b-4bfc-9bb7-74cce43df932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8a948-52a1-452d-ba18-98c21e47bcbf",
   "metadata": {},
   "source": [
    "To facilitate transitions and later computation, each state is represented internally as a pair of $(i,j)$ coordinates in a`numpy np.array`. Externally a state is just an integer 0...15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28678324-56bb-4799-872b-239ecfa7a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = 4\n",
    "terminal_states = (0, world_size**2 - 1)\n",
    "states = {i: np.array([i // world_size, i % world_size]) for i in range(world_size * world_size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eee91446-5b21-441d-bf62-9e55fe44034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {'up':np.array([-1,0]),\n",
    "           'down':np.array([1,0]),\n",
    "           'right':np.array([0,1]),\n",
    "           'left':np.array([0,-1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4bbcf16-8b42-4550-9723-299b408a0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrows = {'left':\"ðŸ ˆ\",\n",
    "          'right':\"ðŸ Š\",\n",
    "          'up':\"ðŸ ‰\",\n",
    "          'down':\"ðŸ ‹\",\n",
    "          'none':\" \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8124453b-b098-4c8b-ba4f-e036595d0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(state, action):\n",
    "    \"\"\" Return the new state resulting from applying action to state.\n",
    "        If action is invalid or if state is terminal, return the input state \"\"\"\n",
    "    if state in terminal_states:\n",
    "        return state\n",
    "    temp = states[state] + actions[action]\n",
    "    # if valid move\n",
    "    if (np.all( np.array([0,0]) <= temp) and np.all(temp < np.array([world_size,world_size]))):\n",
    "        return temp[0]*world_size + temp[1]\n",
    "    else:\n",
    "        # if invalid\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58bf8c8-52b2-4745-974e-3a93e07778b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 7, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick unit test\n",
    "transition(3,'up'), transition(3,'down'), transition(8,'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d432d8aa-5e0d-4773-9324-2e350b55a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(state, action):\n",
    "    \"\"\" Return -1 for all actions from all states \"\"\"\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41d6e1-3846-46ed-bc73-5ec3dbe4dd77",
   "metadata": {},
   "source": [
    "The global `state_actions` maps states to valid actions from that state. There are no actions available from a terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b4088ad-e1a1-4af9-8afb-fe05fd5df58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_actions = dict()\n",
    "\n",
    "for s in states.keys():\n",
    "    children = set()\n",
    "    for a in actions.keys():\n",
    "        if transition(s,a) != s:\n",
    "            children.add(a)\n",
    "    state_actions.update({ s: children } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c20d9f-1aae-4138-ac59-7c3bb8b96b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: set(),\n",
       " 1: {'down', 'left', 'right'},\n",
       " 2: {'down', 'left', 'right'},\n",
       " 3: {'down', 'left'},\n",
       " 4: {'down', 'right', 'up'},\n",
       " 5: {'down', 'left', 'right', 'up'},\n",
       " 6: {'down', 'left', 'right', 'up'},\n",
       " 7: {'down', 'left', 'up'},\n",
       " 8: {'down', 'right', 'up'},\n",
       " 9: {'down', 'left', 'right', 'up'},\n",
       " 10: {'down', 'left', 'right', 'up'},\n",
       " 11: {'down', 'left', 'up'},\n",
       " 12: {'right', 'up'},\n",
       " 13: {'left', 'right', 'up'},\n",
       " 14: {'left', 'right', 'up'},\n",
       " 15: set()}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985188a-82a7-4ba1-bc52-ef9b090149be",
   "metadata": {},
   "source": [
    "The global policy `pi` ($\\pi$) starts out as a uniform policy. Each action is taken with uniform probability from each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa2e4f2d-fbfd-4ba2-a1c6-9cb9eea6197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = dict()\n",
    "for s in states:\n",
    "    children = state_actions[s]\n",
    "    for a in children:\n",
    "        pi.update( { (s,a) : 1/len(children) } )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c75bb4-21f3-4ae3-b47c-c7d6c35de90d",
   "metadata": {},
   "source": [
    "Initialize transition probabilities. In conformity to the RL textbook, $P(s',r | s,a)$ is the defining probability in all problems. Given an agent in state $s$ and completing action $a$, what is the probability of transitioning to state $s'$ and receiving reward $r$. For gridworld, the environment is entirely deterministic. Action $a$ from state $s$ will always transition to the same $s'$ and return reward $-1$. This dictionary contains all valid transitions and maps them to the probability $p=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57151cb3-0a8a-42d0-83b4-9c008266a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b93107e-e418-4e95-8c79-2f6a04e59eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in states:\n",
    "    children = state_actions[s]\n",
    "    for a in children:\n",
    "        next_s = transition(s, a)\n",
    "        r = reward(s, a)\n",
    "        p.update( { (s, a, next_s, r) : 1 } )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b95f8f6-8dc5-48c8-9635-ab6919ee89f2",
   "metadata": {},
   "source": [
    "## 4.1 Iterative Policy Evaluation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce06bdf-375d-456a-8260-6d8383374055",
   "metadata": {},
   "source": [
    "Following the algorithm on p.75 of the RL text, we iteratively evaluate the value of policy `pi` = $\\pi_U$, the uniform policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10b490ec-2499-4f0d-83f2-704062fffd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = { s:0.0 for s in states }\n",
    "gamma = 0.99\n",
    "count = 1\n",
    "while count < 50:\n",
    "    delta = 0\n",
    "    for s in states:\n",
    "        v = V[s]\n",
    "        new_v = 0\n",
    "        for a in state_actions[s]:\n",
    "            r = reward(s, a)\n",
    "            new_state = transition(s, a)\n",
    "            new_v  += pi[(s,a)]*p[(s,a,new_state,r)]*(r + gamma*V[new_state])\n",
    "        V[s] = new_v\n",
    "        delta = max(delta, abs(v - V[s]))\n",
    "    if delta < 0.01:\n",
    "        break\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76660a20-3aa3-470b-a042-5143ff7cc93f",
   "metadata": {},
   "source": [
    "Now compute the optimal policy based on the child with max V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c5daba0-dc16-482b-86d2-94bd3dfdea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal = dict()\n",
    "for s in states:\n",
    "    children = state_actions[s]\n",
    "    children_states = [transition(s,a) for a in children]\n",
    "    if (len(children_states) == 0):\n",
    "        continue\n",
    "    best_child = max([c for c in children_states], key = lambda x: V[x])\n",
    "    optimal.update({s : best_child} )\n",
    "optimal.update({ s: s for s in terminal_states })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cbac9c1-164f-45b8-add4-8266acb428aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00    -9.67   -13.58   -14.44 \n",
      "   -9.67   -12.72   -14.02   -13.59 \n",
      "  -13.58   -14.02   -12.73    -9.69 \n",
      "  -14.44   -13.59    -9.69     0.00 \n"
     ]
    }
   ],
   "source": [
    "for i in range(world_size):\n",
    "    for j in range(world_size):\n",
    "        print (f\"{V[i*world_size + j]:8.2f}\", end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "694fc3f6-38bf-403d-873b-652e5fdbad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    0    1    2 \n",
      "   0    4    5   11 \n",
      "   4    5   11   15 \n",
      "   8   14   15   15 \n"
     ]
    }
   ],
   "source": [
    "for i in range(world_size):\n",
    "    for j in range(world_size):\n",
    "        print (f\"{optimal[i*world_size + j]:4}\", end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5e56c-4d0e-41d4-b787-47ab1ab0c282",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "c043870c-7f4c-4297-b6e5-e0f94f566009",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = dict( {s : random.random() for s in states} )\n",
    "V.update ( {s : 0 for s in terminal_states } )\n",
    "pi = dict( {s : random.choice(list(state_actions[s])) for s in states if len(state_actions[s]) > 0} )\n",
    "pi.update( { s: None for s in terminal_states} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "3c0a0a5a-2ff1-4234-a303-5bb570055b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "help: 1 -1.0 0 -1 0.4 0\n"
     ]
    }
   ],
   "source": [
    "count= 0\n",
    "gamma = 0.4\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v = V[s]\n",
    "            if pi[s]:\n",
    "                next_s = transition(s,pi[s])\n",
    "                r = reward(s, pi[s])\n",
    "                if (s == 1):\n",
    "                    print (\"help:\", s, v, next_s, r, gamma, V[next_s])\n",
    "                V[s] = 1 * (r + gamma * V[next_s])\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "                #print(delta, s, V[s])\n",
    "        if delta < 2:\n",
    "            break\n",
    "    done = True\n",
    "    #print(pi)\n",
    "    optimal = dict()\n",
    "    for s in states:\n",
    "        if s not in terminal_states:\n",
    "            children = state_actions[s]\n",
    "            best_child = max([c for c in children], \\\n",
    "                             key = lambda x: (-1+gamma*V[transition(s,x)]))\n",
    "            optimal.update({s : best_child} )\n",
    "            if optimal[s] != pi[s]:\n",
    "                done = False\n",
    "                pi[s] = optimal[s]\n",
    "optimal.update({ s: 'none' for s in terminal_states })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "efa776b6-8a60-4171-a88a-474e60e40de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 15)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "b3d4fb95-92e8-41d2-94d4-a029eba0103c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ðŸ ˆ  ðŸ ˆ  ðŸ ˆ  \n",
      "ðŸ ‰  ðŸ ˆ  ðŸ Š  ðŸ ‹  \n",
      "ðŸ ‰  ðŸ Š  ðŸ Š  ðŸ ‹  \n",
      "ðŸ Š  ðŸ Š  ðŸ Š     \n"
     ]
    }
   ],
   "source": [
    "for i in range(world_size):\n",
    "    for j in range(world_size):\n",
    "        print (f\"{arrows[optimal[i*world_size + j]]:2}\", end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "03310102-9a80-4467-9ccd-c2b3e550ac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0   -1.0   -1.4  -1.56 \n",
      "  -1.0   -1.4  -1.56   -1.4 \n",
      "  -1.4  -1.56   -1.4   -1.0 \n",
      " -1.56   -1.4   -1.0      0 \n"
     ]
    }
   ],
   "source": [
    "for i in range(world_size):\n",
    "    for j in range(world_size):\n",
    "        print (f\"{V[i*world_size + j]:6}\", end=' ')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
